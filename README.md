# Awesome Embodied AI [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

**Contributors:** todo

## Survey
- Foundation Models in Robotics: Applications, Challenges, and the Future [[paper]](https://arxiv.org/pdf/2312.07843)
- Foundation Models for Decision Making: Problems, Methods, and Opportunities [[paper]](https://arxiv.org/pdf/2303.04129)

## Large Language Models (LLMs)
- Awesome-LLM [[project]](https://github.com/Hannibal046/Awesome-LLM)
- GPT-3: Language Models are Few-Shot Learners [[paper]](https://arxiv.org/abs/2005.14165)
- GPT-4: GPT-4 Technical Report [[project]](https://openai.com/research/gpt-4)
- LLaMA: Open and Efficient Foundation Language Models [[paper]](https://arxiv.org/abs/2302.13971)
- Llama 2: Open Foundation and Fine-Tuned Chat Models [[paper]](https://arxiv.org/abs/2307.09288)
- Mistral 7B [[paper]](https://arxiv.org/pdf/2310.06825.pdf%5D%5D%3E)

## Vision-Language Models (VLMs)

### Image-Language Models
- BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation [[paper]](https://proceedings.mlr.press/v162/li22n.html)
- BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [[paper]](https://proceedings.mlr.press/v202/li23q.html)
- Learning Transferable Visual Models From Natural Language Supervision [[paper]](https://proceedings.mlr.press/v139/radford21a)
- Visual Instruction Tuning [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html)
- Improved Baselines with Visual Instruction Tuning [[paper]](https://arxiv.org/abs/2310.03744)
- Flamingo: a Visual Language Model for Few-Shot Learning [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)
- LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention [[paper]](https://arxiv.org/abs/2303.16199)
- PandaGPT: One Model To Instruction-Follow Them All [[paper]](https://arxiv.org/abs/2305.16355)
- OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models [[paper]](https://arxiv.org/abs/2308.01390)
- InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html)
- mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality [[paper]](https://arxiv.org/abs/2304.14178)
- MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models [[paper]](https://arxiv.org/abs/2304.10592)
- ShareGPT4V: Improving Large Multi-Modal Models with Better Captions [[paper]](https://arxiv.org/abs/2311.12793)

### Video-Language Models
- Learning Video Representations from Large Language Models [[paper]](https://arxiv.org/abs/2212.04501)
- VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset [[paper]](https://arxiv.org/abs/2305.18500v1)
- Otter: A Multi-Modal Model with In-Context Instruction Tuning [[paper]](https://arxiv.org/abs/2306.05425)
- Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks [[paper]](https://arxiv.org/abs/2306.04362)
- Valley: Video Assistant with Large Language model Enhanced abilitY [[paper]](https://arxiv.org/abs/2306.07207)
- Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration [[paper]](https://arxiv.org/abs/2306.09093)
- World Model on Million-Length Video And Language With Blockwise RingAttention [[paper]](https://arxiv.org/abs/2402.08268)
- Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding [[paper]](https://arxiv.org/abs/2311.08046)
- LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models [[paper]](https://arxiv.org/abs/2311.17043)
- VideoChat: Chat-Centric Video Understanding [[paper]](https://arxiv.org/abs/2305.06355)
- Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding [[paper]](https://arxiv.org/abs/2306.02858)
- Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models [[paper]](https://arxiv.org/abs/2306.05424)
- Video-LLaVA: Learning United Visual Representation by Alignment Before Projection [[paper]](https://arxiv.org/abs/2311.10122)
- PG-Video-LLaVA: Pixel Grounding Large Video-Language Models [[paper]](https://arxiv.org/abs/2311.13435v2)
- GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation [[paper]](https://arxiv.org/abs/2311.16511v1)

## Simulators
- VirtualHome: Simulating Household Activities via Programs [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html)
- Gibson Env: Real-World Perception for Embodied Agents [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html)
- iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks [[paper]](https://arxiv.org/abs/2108.03272)
- Habitat: A Platform for Embodied AI Research [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html)
- Habitat 2.0: Training Home Assistants to Rearrange their Habitat [[paper]](https://proceedings.neurips.cc/paper/2021/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html)
- Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots [[paper]](https://arxiv.org/abs/2310.13724)
- AI2-THOR: An Interactive 3D Environment for Visual AI [[paper]](https://arxiv.org/abs/1712.05474)
- RoboTHOR: An Open Simulation-to-Real Embodied AI Platform [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Deitke_RoboTHOR_An_Open_Simulation-to-Real_Embodied_AI_Platform_CVPR_2020_paper.html)
- BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation [[paper]](https://proceedings.mlr.press/v205/li23a.html)
- ThreeDWorld：A High-Fidelity, Multi-Modal Platform for Interactive Physical Simulation [[paper]]("https://arxiv.org/abs/2007.04954")
- LIBERO: Benchmarking Knowledge Transfer in Lifelong Robot Learning [[paper]](https://arxiv.org/pdf/2306.03310.pdf)
- ProcTHOR: Large-Scale Embodied AI Using Procedural Generation [[paper]](https://arxiv.org/abs/2206.06994)
- PyBullet：physics simulation for games, visual effects, robotics and reinforcement learning. [[paper]](https://pybullet.org/wordpress/)

## Video Data
- Scaling Egocentric Vision: The EPIC-KITCHENS Dataset [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html)
- Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100 [[paper]](https://link.springer.com/article/10.1007/s11263-021-01531-2)
- Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives [[paper]](https://arxiv.org/abs/2311.18259)
- BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation [[paper]](https://arxiv.org/abs/2403.09227)
- Ego4D: Around the World in 3,000 Hours of Egocentric Video [[paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.html)
- Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos [[paper]](https://arxiv.org/abs/1804.09626)
- Delving into Egocentric Actions [[paper]](https://openaccess.thecvf.com/content_cvpr_2015/html/Li_Delving_Into_Egocentric_2015_CVPR_paper.html)

## Egocentric
- Ego-Topo: Environment Affordances From Egocentric Video [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.html)

## High-Resolution
- OtterHD: A High-Resolution Multi-modality Model [[paper]](https://arxiv.org/pdf/2311.04219)

## EAI with Foundation Models

- 3D-LLM: Injecting the 3D World into Large Language Models [[paper]](https://arxiv.org/abs/2307.12981)
- Reward Design with Language Models [[paper]](https://arxiv.org/abs/2303.00001)
- Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [[paper]](https://proceedings.mlr.press/v205/ichter23a.html)
- Inner Monologue: Embodied Reasoning through Planning with Language Models [[paper]](https://arxiv.org/abs/2207.05608)
- Text2Motion: from natural language instructions to feasible plans [[paper]](https://link.springer.com/article/10.1007/s10514-023-10131-7)
- VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models [[paper]](https://arxiv.org/abs/2307.05973)
- ProgPrompt: Generating Situated Robot Task Plans using Large Language Models [[paper]](https://ieeexplore.ieee.org/abstract/document/10161317)
- Code as Policies: Language Model Programs for Embodied Control [[paper]](https://ieeexplore.ieee.org/abstract/document/10160591)
- ChatGPT for Robotics: Design Principles and Model Abilities [[paper]](https://arxiv.org/abs/2306.17582)
- LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action [[paper]](https://proceedings.mlr.press/v205/shah23b.html)
- Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html)
- L3MVN: Leveraging Large Language Models for Visual Target Navigation [[paper]](https://ieeexplore.ieee.org/abstract/document/10342512)
- HomeRobot: Open-Vocabulary Mobile Manipulation [[paper]](https://arxiv.org/abs/2306.11565)
- RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation [[paper]](https://arxiv.org/abs/2306.11706)
- Statler: State-Maintaining Language Models for Embodied Reasoning [[paper]](https://arxiv.org/abs/2306.17840)
- Collaborating with language models for embodied reasoning [[paper]](https://arxiv.org/abs/2302.00763)
- EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4ec43957eda1126ad4887995d05fae3b-Abstract-Conference.html)
- MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/74a67268c5cc5910f64938cac4526a90-Abstract-Datasets_and_Benchmarks.html)
- Voyager: An Open-Ended Embodied Agent with Large Language Models [[paper]](https://arxiv.org/abs/2305.16291)
- Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory [[paper]](https://arxiv.org/abs/2305.17144)
- Guiding Pretraining in Reinforcement Learning with Large Language Models [[paper]](https://proceedings.mlr.press/v202/du23f.html)
- Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents [[paper]](https://proceedings.mlr.press/v162/huang22a.html)


## Embodied Multi-modal Language Models

### Representing Learning
- Language-Driven Representation Learning for Robotics [[paper]](https://arxiv.org/abs/2302.12766)
- R3M: A Universal Visual Representation for Robot Manipulation [[paper]](https://arxiv.org/abs/2203.12601)
- VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training [[paper]](https://arxiv.org/abs/2210.00030)
- LIV: Language-Image Representations and Rewards for Robotic Control [[paper]](https://proceedings.mlr.press/v202/ma23b.html)
- Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation [[paper]](https://proceedings.mlr.press/v164/nair22a.html)
- DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning [[paper]](https://arxiv.org/abs/2402.18137)

### End-to-End
- Masked Visual Pre-training for Motor Control [[paper]](https://arxiv.org/abs/2203.06173)
- Real-World Robot Learning with Masked Visual Pre-training [[paper]](https://proceedings.mlr.press/v205/radosavovic23a.html)
- RT-1: Robotics Transformer for Real-World Control at Scale [[paper]](https://arxiv.org/abs/2212.06817)
- RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [[paper]](https://proceedings.mlr.press/v229/zitkovich23a.html)
- Open X-Embodiment: Robotic Learning Datasets and RT-X Models [[paper]](https://arxiv.org/abs/2310.08864)
- PaLM-E: An Embodied Multimodal Language Model [[paper]](https://arxiv.org/abs/2303.03378)
- PaLI-X: On Scaling up a Multilingual Vision and Language Model [[paper]](https://arxiv.org/abs/2305.18565)
- A Generalist Agent [[paper]](https://arxiv.org/abs/2205.06175)


## Benchmarks
